[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17644837.svg)](https://doi.org/10.5281/zenodo.17644837)
## ğŸ“˜ Citation
If you use this work, please cite:
**Konidina, Sri Charan (2025). _Retail AI Analytics Whitepaper â€” Anomaly Detection, Real-Time Retail ML, and AI Recruitment Systems._ Zenodo. https://doi.org/10.5281/zenodo.17644837**

# ğŸ¤– AI Recruitment Suite â€” v1.1

An end-to-end AI-powered recruitment system that automates resume parsing, skill extraction,
experience similarity scoring, and unified candidate evaluation using explainable AI techniques.

**Release Version:** v1.1  
**Author:** Sri Charan Konidina  
**Tech Stack:** Python, Streamlit, NLP, TF-IDF, LLMs  

---

## ğŸš€ Whatâ€™s New in v1.1
- Job Description Classification
- Resume Parsing (PDF + Text)
- Skill Extraction Engine
- Experience Similarity Scoring (TF-IDF)
- Unified Candidate Scoring Engine
- Interactive Streamlit Dashboards

---

# Retail Anomaly Detection - Proof of Concept (PoC)

This demo detects unusual sales or inventory patterns using Isolation Forest (AI-based anomaly detection).

**Author:** Sri Charan Chowdary Konidina  
**Start Date:** Oct 7, 2025  
**Tech:** Python, Scikit-learn, Pandas, NumPy

### Objective
Help retailers identify abnormal spikes or drops in sales.

### How It Works
1. Load a CSV dataset.
2. Train IsolationForest model.
3. Detect anomalies.
4. Print anomaly indices.

### Example Output
Anomalies found:
    day  sales  anomaly
14   15    300       -1
18   19   1005       -1
24   25    500       -1


### Future Plans
- Add visualization (matplotlib)
- Add dashboard UI (Streamlit)
- Publish article on Medium / LinkedIn

### Day 1 Update â€” Oct 8, 2025
- Added data.csv file for testing.
- Implemented matplotlib visualization for anomaly detection.
- Project now displays a graph with red dots marking anomalies.

## ğŸ§  Day 2 â€” Interactive Anomaly Visualization (Plotly)

- Built an interactive anomaly detection chart using Plotly.
- Visualizes normal vs anomaly points on a dynamic graph.
- Technologies used: Python, Pandas, Scikit-Learn, Plotly.


### Example Output:
Anomalies found (rows):
 day  sales
  12    300
  18   1000
  24    500

### ğŸ§© What I Did Today
- Installed and configured **Plotly** for interactive visualization.
- Created a script (`visual_demo.py`) to visualize anomaly detection results from my Isolation Forest model.
- Successfully plotted **sales trends** and highlighted anomalies dynamically using red markers.

### ğŸ“Š What I Learned
- How visualization helps explain model results â€” not just raw outputs.
- Understood how **interactive graphs** (Plotly) make anomaly detection results easier to interpret.
- Learned that visual evidence of insights is a strong part of building credible AI/analytics portfolios.

### âš™ï¸ Technical Takeaways
- Learned the use of `plotly.express` for line plots and scatter overlays.
- Integrated anomaly data from `IsolationForest` predictions into a chart.
- Reinforced concepts of contamination ratio and anomaly labeling.

### ğŸ§  Why This Matters (O-1 Relevance)
- This shows my **ability to communicate machine learning insights** visually â€” a skill valuable for research, analytics, and AI engineering roles.
- Contributes to the â€œoriginal contribution / impactful demonstrationâ€ criterion by producing **publicly verifiable code** and outputs.

---

### ğŸ§­ Day 3 â€” Retail Anomaly Detection (Notebook + Report)
**Goal:** Convert anomaly detection script into a Jupyter Notebook and PDF report.  
**What I Did:**  
- Created `retail_anomaly_demo.ipynb` using Isolation Forest.  
- Visualized anomalies using Matplotlib.  
- Exported a clean PDF report for documentation.  
**Output Files:**  
- `retail_anomaly_demo.ipynb`  
- `retail_anomaly_demo_report.pdf`  
**Result:** Successfully demonstrated anomaly detection and documented results visually.

---
### ğŸ§­ Day 4
**Goal:** Use a real dataset from Kaggle to perform retail sales analysis and detect anomalies with Isolation Forest.

**ğŸ§  Tasks Done**
- Downloaded the Supermarket Sales dataset from Kaggle and saved as sales.csv.
- Loaded the dataset in a Jupyter Notebook using python -m notebook.
- Performed EDA (Exploratory Data Analysis) and data cleaning.
- Visualized sales distribution and product line revenues with matplotlib & seaborn.
- Applied Isolation Forest to detect sales anomalies.
- Highlighted anomalous transactions on a time-series plot.
- Generated a PDF report from the HTML output and committed to GitHub.
**ğŸ“Š Key Visuals & Insights**
- Histogram: Distribution of Total Sales values.
- Bar Chart: Total Sales by Product Line (ranked highest to lowest).
- Boxplot: Sales distribution by weekday.
- Time-Series Plot: Anomalies highlighted in red over sales timeline.
- Summary: Anomalies â‰ˆ 1 % of transactions (rare but significant).


### ğŸ§­ Day 5
**Goal:** Publish the project publicly on LinkedIn and Medium to showcase the results and share learnings.
**ğŸ§  Tasks Done**
- Selected 2 visuals from Day 4 (Bar Chart + Anomaly Plot) for publication.
- Posted a short summary of the Retail Anomaly Detection PoC on LinkedIn.
- Published a detailed article on Medium explaining the process, dataset, and results.
- Added both post links and screenshots to the evidence sheet.
# LinkedIn
# â†’ created post using provided Day 5 text and attached images.
# Medium
# â†’ created new story, pasted full article text, added charts, published.
# Evidence updated in spreadsheet
**âœ… Outcome**
- Demonstrated public communication and dissemination skills.
- Showcased the project to a professional audience on both LinkedIn and Medium.
- Prepared the project for portfolio and final O1 submission.

## ğŸ”¬ **Day 6 â€” Real-Time Retail Streaming Anomaly Detection**

**Date:** 2025-10-13  
**Goal:** Simulate real-time data flow and detect anomalies dynamically.

### ğŸ§  Tasks Done
- Created streaming data simulation for 50 days of retail sales.  
- Implemented live anomaly detection with Isolation Forest.  
- Visualized live updates in Jupyter Notebook.  
- Exported HTML report and committed to GitHub.
### ğŸ“Š Key Outputs
- Animated time-series anomaly chart
- HTML report
- GitHub commit proof

## ğŸ§­ Day 7 â€” Mini Research Paper & Contribution Packet

- ğŸ“… Date: October 14, 2025
- ğŸ¯ Goal: Consolidate all previous work (Days 0â€“6) into a short research-style â€œmini paperâ€ â€” a reproducible artifact showing your      ability to plan, execute, and document an ML project end-to-end.
## ğŸ§© Steps Completed
- Created mini_paper.md
- Built converter script using Pythonâ€™s markdown library to generate a polished HTML report.
- Committed & pushed all artifacts to GitHub:
    - mini_paper.md
    - mini_paper.html
    - mini_paper_report.pdf
    - convert_md_to_html.py
- Captured screenshots for Drive evidence: HTML preview, PDF report, GitHub commit, and final folder structure.
- Updated O1 Evidence Sheet with entry ID 7 â€” â€œMini Paper: Retail Anomaly Detection.â€
## ğŸ§  Key Learnings
- Converted research progress into a citable, shareable artifact (important for O-1 visa documentation).
- Learned Markdown-to-HTML/PDF conversion and lightweight report formatting without LaTeX.
- Understood how to summarize ML projects into structured technical documentation.
- Practiced creating reproducible public deliverables (GitHub + Drive + Evidence Index).
## âœ… Outcome
- A professional mini research paper summarizing all prior project work â€” establishing a strong, documented technical contribution.

## ğŸ§­ Day 8 â€” Publish Project on GitHub Pages  
**ğŸ“… Date:** October 16, 2025  
**ğŸ¯ Goal:** Make anomaly detection project publicly accessible online through GitHub Pages â€” turning mini research project into a live interactive portfolio artifact.  
### ğŸ§© Step Summary  
- Converted project notebook and summary into a clean HTML page.  
- Added `index.html` to the repository root for GitHub Pages.  
- Configured GitHub Pages to deploy from the **main branch (root folder)**.  
- Verified that the site loads properly online.  
- Documented the project as an official **â€œpublic evidenceâ€** link.  
### ğŸ§  What Was Done  
- Generated a readable HTML report version of the project using:  
  ```bash
- notebook index.html
### ğŸ§  Outcome
- Project is now publicly accessible online.
- Acts as an independent, timestamped record of your original work.
- Strengthens my O-1 â€œoriginal contributionâ€ and â€œpublic disseminationâ€ evidence.

### ğŸ§­ Day 9 â€” Interactive Retail Anomaly Dashboard (Streamlit)
**Date:** October 16, 2025  
**Goal:** Created an interactive Streamlit dashboard for anomaly detection.  
**Highlights:**
- Added a web-based UI for users to upload CSVs and analyze retail data.  
- Integrated Isolation Forest to mark anomalies visually.  
- Dynamic contamination slider for flexible model tuning.  
**Output:**
Interactive dashboard visible at http://localhost:8501  
Displays anomaly points (in red) and summary table.

## ğŸ§­ Day 10 â€” Retail Anomaly Interactive Dashboard
**Goal: Build a live interactive dashboard that detects anomalies from any uploaded CSV file.**
## What I Did: 
- Created retail_anomaly_dashboard.py using Streamlit for real-time interaction.
- Added file upload functionality to analyze any dataset dynamically.
- Implemented Isolation Forest to detect anomalies instantly upon upload.
- Visualized anomalies with red markers on dynamic charts and displayed a summary table.
## Output Files:
- retail_anomaly_dashboard.py
- Result: Built a fully interactive AI dashboard where users can upload datasets, visualize anomalies live, and view detailed detection summaries â€” showcasing real-world machine learning and dashboard development skills.

### ğŸ§­ Day 11 â€” AI Resume Analyzer (Streamlit)
**Goal:** Create an AI-powered app that scores resumes for AI/ML readiness potential.  
**What I Did:**  
- Built a Streamlit app that analyzes resume text using NLP-based keyword detection.  
- Added scoring logic for AI keywords and achievements.  
- Produced a dynamic feedback interface with recommendations.  
**Output Files:**  
- `resume_analyzer_app.py`  
**Result:** Working AI app that evaluates resumes and provides resume score  feedback interactively.

### ğŸ§  Day 12 â€” Resume Analyzer Visualization & Download Report
**Goal:** Enhance the AI Resume Analyzer with visual insights and a downloadable report.  
**What I Did:**  
- Added interactive keyword frequency visualization (Matplotlib).  
- Displayed achievement-related terms in a clean dataframe.  
- Enabled one-click `.txt` report download.  
**Output Files:**  
- `resume_analyzer_app.py`  
- `resume_analysis_report.txt`  
**Result:** Built a more user-friendly and data-driven resume analyzer that provides both insights and exportable documentation.

### ğŸ§­ Day 13 â€” AI Resume Analyzer Enhancement (Skill Extraction & Scoring)
**Goal:** Extend the AI Resume Analyzer by adding NLP-powered skill detection and match scoring.
**Tasks Completed:**
- Integrated spaCy for keyword-based skill extraction
- Designed an automatic resume skill scoring engine
- Created Streamlit-based visual interface with score chart
- Uploaded TXT resume, extracted key skills, and visualized presence
- Added results and screenshots to evidence repository
**Evidence Added:**
- `resume_analyzer_enhanced.py`
- `resume_analyzer_enhanced_screenshot1.png`
- `resume_analyzer_enhanced_screenshot2.png`
**Outcome:** Demonstrated AI-based resume scoring and visualization, progressing from prototype to intelligent NLP-enabled system.

---

### ğŸ§­ Day 14 â€” Resume Comparison Dashboard (Multi-Resume AI Analyzer)
**Goal:** Build a multi-resume AI dashboard that compares skill match across multiple candidates.
**Tasks Completed:**
- Developed Streamlit app for multiple resume uploads
- Used spaCy NLP for skill extraction and scoring
- Created comparison heatmap using Seaborn and Matplotlib
- Visualized candidate skill match percentages and best-fit ranking
**Evidence Added:**
- `resume_comparison_dashboard.py`
**Outcome:** Demonstrated multi-document AI comparison, showing the ability to scale single-resume NLP analysis into a full comparative analytics dashboard.

---

### ğŸ§­ Day 15 â€” Resume Summary Generator (OpenAI Integration)
**Goal:** Extend the AI Resume Analyzer to generate professional summaries and export them as PDFs.

**Tasks Completed:**
- Integrated OpenAI GPT model for text generation
- Created Streamlit app for single-resume summarization
- Automated PDF export using FPDF library
- Enhanced AI pipeline for real-world HR automation

**Evidence Added:**
- `resume_summary_generator.py`
- Screenshots (Summary UI, PDF Export)
- `.env` configuration for secure API usage

**Outcome:** Demonstrated generative AI capability by transforming raw resumes into structured, downloadable summaries â€” proving end-to-end NLP automation and product scalability.

## Day 16 â€” AI Resume Matching System Integration

### Description
- Integrated **AI Resume Matching System** that ranks multiple resumes against a job description.
- Generates **match score and reasoning** using GPT-4 model.
- Added **visual analytics (bar chart)** to show ranking.
- This enhancement evolves the Resume Analyzer into a **full AI Recruitment Assistant** capable of candidate screening automation.

### Evidence
- File: `resume_matcher.py`
- Output: Resume match scores printed in terminal
- Visualization: Matplotlib bar chart generated locally

## Day 17 â€” AI Recruitment Insights Dashboard
**Type:** AI Application Enhancement  
**URL:** [GitHub Repository](https://github.com/konidinasricharan/retail-anomaly-poc)

### Description
- Extended the **AI Recruitment Assistant** into a **Recruitment Insights Dashboard**.
- Added keyword extraction, skill coverage chart, and AI-generated pool analysis.
- Integrated **Matplotlib** for interactive charts inside Streamlit.
- Provides recruiters with a visual and narrative overview of candidate quality and skill trends.

### Evidence
- File: `resume_insight_dashboard.py`  
- Run Command: `streamlit run resume_insight_dashboard.py`  
- Output: Interactive dashboard with candidate insights, skill analytics, and AI-generated recommendations.

## Day 18 â€” AI Resume Ranking & Feedback System  
**Type:** AI Application Enhancement  
**URL:** [GitHub Repository](https://github.com/konidinasricharan/retail-anomaly-poc)

### Description
- Built a Resume Ranking and Feedback System extending the AI Recruitment Assistant.  
- Implements automated ranking, personalized feedback, and CSV export.  
- Provides visual candidate score distribution and recruiter recommendations via AI.  

### Evidence
- File: `resume_ranking_system.py`  
- Run: `streamlit run resume_ranking_system.py`  
- Output: Ranked dashboard with AI feedback and CSV report.  

## Day 19 â€” AI Shortlisting Assistant with Job Fit Reasoning
**Type:** AI Explainability & Decision Support  

### Description
- Added Smart Shortlisting Assistant to automatically shortlist candidates using AI reasoning.  
- Introduced threshold-based selection and AI explanations for each shortlisted candidate.  
- Implemented exportable CSV for recruiter reports and integrated recruiter summary generation.

### Evidence
- File: `ai_shortlisting_assistant.py`  
- Run: `streamlit run ai_shortlisting_assistant.py`  
- Output: Shortlist reasoning dashboard + CSV export  

### Day 20 â€” AI Recruitment Analytics Dashboard

- Goal: Extend the resume shortlisting system into a full analytics dashboard that provides insights, score distribution, keyword trends, and generates an AI Hiring Summary report.

### What was done today
- Created a new Streamlit app: ai_recruitment_dashboard.py
- Added CSV upload support for shortlisted_candidates.csv
- Extracted and cleaned score values for analysis
- Implemented a score distribution bar chart
- Extracted frequent words from AI Analysis to identify common skill themes
- Added a button to generate AI Hiring Summary Report
- Automatically saves the summary to:
- ai_hiring_summary.txt
- Prepared the dashboard as an advanced stage recruitment analytics tool
### Files Added
- ai_recruitment_dashboard.py
- ai_hiring_summary.txt (auto-generated after summary step)
### Evidence Added
- Screenshot of dashboard interface
- Screenshot of score chart + keyword chart
- AI Hiring Summary text file added to Google Drive

---

### ğŸ§­ Day 21 â€” Whitepaper: Retail AI Analytics & Recruitment Assistant
**Goal:** Publish a technical whitepaper that consolidates Days 0â€“20 into a reproducible research-style report.

**Files Added:**
- `Retail_AI_Analytics_Whitepaper.md`
- `Retail_AI_Analytics_Whitepaper.pdf`

**Summary:**  
This whitepaper documents the development and evaluation of:
1. A Retail Anomaly Detection proof-of-concept (Isolation Forest + streaming simulation).  
2. An AI Recruitment Assistant (resume scoring, comparison, shortlisting, and analytics dashboards).  
It contains methodology, dataset descriptions, experimental results, visualizations, and next steps.

**Evidence:** Public PDF on GitHub + GitHub Release (optionally archived on Zenodo for DOI).

## ğŸ“… Day 22 â€” Paper #2: AI Recruitment System Technical Deep-Dive  
**Date:** November 19 2025  
**Type:** Technical Paper / Documentation  

### ğŸ“ Summary  
Today, I authored **Paper #2 â€” "AI Recruitment System: Architecture, Evaluation & LLM-Based Ranking"**, a full technical deep-dive describing how my AI recruitment system works end-to-end.  
This paper includes:

- Full system architecture  
- Resume ingestion â†’ Job description parsing â†’ LLM ranking pipeline  
- Score extraction logic  
- Streamlit analytics dashboard  
- Evaluation methodology  
- Prompt engineering strategy  
- Future improvements  
- Real ATS applicability  

This serves as strong **Evidence** under:  
âœ”ï¸ Original Contribution  
âœ”ï¸ Authorship  
âœ”ï¸ Evidence of Expertise  
âœ”ï¸ Technical Documentation  

### ğŸ“‚ Files Added  
- `AI_Recruitment_Technical_Paper.md`  
- `assets/day23/` (optional screenshots)  

### ğŸ“Œ Steps Completed  
- Wrote full 9-section technical paper  
- Added Markdown file to GitHub  
- Updated evidence spreadsheet  
- Prepared content for future DOI submission / publication  

## ğŸ“… Day 24 â€” Medium Article #2 (AI Recruitment)

**Task:** Published a full Medium article titled *â€œAI in Recruitment: How Resume Scoring & Hiring Analytics Transform Talent Selectionâ€*.

**Why this matters:**  
This article demonstrates public dissemination of expertise and supports authorship & original contributions.

**Link:** https://medium.com/@konidinasricharan/ai-in-recruitment-how-resume-scoring-hiring-analytics-transform-talent-selection-20fb1f0e094c

**Evidence Added:**  
- Screenshots  
- URL  

## ğŸ“… Day 25 â€” LinkedIn Research Thread

**Task:** Published a 7-part LinkedIn research thread titled *â€œ20-Day AI Journey: From Retail Anomalies to AI Recruitment Systems.â€*

**Includes:**
- Breakdown of all projects (Days 0â€“20)
- Public dissemination of technical expertise
- Thread with engagement + screenshots for evidence

**Link:** (https://www.linkedin.com/in/sricharankonidina/)

## ğŸ“… Day 26 â€” System Architecture Diagrams (Retail + Recruitment)

**Task:** Created two professional architecture diagrams using diagrams.net:
1. Retail Anomaly Detection System
2. AI Recruitment & Resume Scoring System

**Outputs Added:**
- PNG + PDF diagrams
- `/architecture/` folder in GitHub

**Why this matters:**  
Architecture diagrams demonstrate technical clarity and are used as evidence for original contributions and engineering documentation.

**Status:** Completed.

## ğŸ“… Day 27 â€” Conference Submission Version (DOI Published)

**Task:**  
Converted the AI Recruitment System whitepaper into a formal *conference-style submission version* and published it with a DOI through Zenodo.  
This counts as **extraordinary ability evidence** under:

- Authorship of scholarly articles  
- Original contributions of major significance  
- Published work in the field  
- Public dissemination of technical research  


**DOI:**  
10.5281/zenodo.17791779

**Files Added to Repo:**  
- `/conference/ai_recruitment_conference_paper.pdf`  
- `/conference/ai_recruitment_conference_submission.zip`  

**Why this matters:**  
A conference submission + DOI is considered formal research dissemination and strengthens multiple criteria. This shows active participation in the scientific community and supports your claim of contributing original work to the field of AI-driven recruitment systems.

**Status:** Completed.

## ğŸ—“ï¸ Day 28 â€” AI Job Description Classifier Module (ML Model + Streamlit UI)

### ğŸ¯ Goal  
Build a machine-learning powered classifier that can automatically categorize job descriptions into job families using TF-IDF + Logistic Regression.  
Add a Streamlit interface for interactive predictions.

This module becomes a key part of the AI Recruitment Suite and counts as *Original Contribution* for evidence.

---

### ğŸ§© What I Built Today
#### 1ï¸âƒ£ **A robust ML training pipeline**
- Automatically creates dataset if missing  
- Trains TF-IDF + Logistic Regression  
- Handles 125+ job description samples across 5 job families  
- Saves model to: `job_classifier/model/job_classifier.pkl`

#### 2ï¸âƒ£ **A production-ready prediction function**
- Handles class errors safely  
- Returns ranked probability table  
- Useful for O-1 â€œoriginal contributionâ€ evidence

Features:
- â€œTrain Model Nowâ€ button  
- Dataset preview  
- Probability ranking table  
- Predicted job family display  
- Debug expander showing training paths, model classes, etc.

## Run command:
## ```bash
## streamlit run job_classifier/job_classifier_streamlit.py.

## ğŸ—“ï¸ Day 29 â€” AI Skill Extraction Engine (NER-Based NLP Module)

### ğŸ¯ Goal  
Build an explainable **Skill Extraction Engine** that automatically identifies technical skills from job descriptions and resumes using lightweight NLP (NER-style keyword matching).

This module strengthens the AI Recruitment Suite by adding transparent skill intelligence used in candidate evaluation and job matching.

---

### ğŸ§© What I Built Today

#### 1ï¸âƒ£ **NER-Style Skill Extraction Engine**
- Rule-based NLP system using curated technical skill vocabulary
- Extracts skills across multiple categories:
  - Programming Languages  
  - Data & Machine Learning  
  - Cloud & DevOps  
  - Data Engineering  
  - Security  
  - Tools
- Designed for explainability and ATS-style interpretation

#### 2ï¸âƒ£ **Reusable Python Extraction Module**
File: job_classifier/skill_extractor.py
- Normalizes text
- Uses regex-based matching
- Returns structured skill categories
- No external NLP dependencies required

#### 3ï¸âƒ£ **Interactive Streamlit Interface**
File: job_classifier/skill_extractor_streamlit.py

Features:
- Paste job description or resume
- One-click skill extraction
- Grouped skill visualization
- Clean recruiter-friendly UI

### Run command:
### ```bash
### streamlit run job_classifier/skill_extractor_streamlit.py

## ğŸ—“ï¸ Day 30 â€” Resume Parsing Engine (PDF + Text Parser)

### ğŸ¯ Goal  
Build a **Resume Parsing Engine** that extracts clean, structured text from PDF resumes and prepares the content for downstream AI modules such as skill extraction, resume scoring, and job matching.

This module forms a core component of an ATS-style AI recruitment system.

---

### ğŸ§© What I Built Today

#### 1ï¸âƒ£ **PDF Resume Parsing Engine**
- Extracts text from multi-page PDF resumes
- Cleans and normalizes raw resume content
- Removes noise, extra spaces, and non-ASCII characters
- Produces usable plain text for NLP pipelines

File:job_classifier/resume_parser.py

---

#### 2ï¸âƒ£ **Interactive Streamlit Resume Upload UI**
File:job_classifier/resume_parser_streamlit.py

Features:
- Upload PDF resumes
- Automatic parsing and extraction
- Scrollable text output for review
- Ready for integration with skill and matching engines

# Run command:
- ```bash
- streamlit run job_classifier/resume_parser_streamlit.py

## ğŸ§  Relevance
- Demonstrates applied NLP and document processing
- Shows real-world ATS system design
- Enables automated resume analysis workflows
- Strengthens sustained original contribution evidence

## ğŸ—“ï¸ Day 31 â€” Resume Skill Extraction (Parsed Resume â†’ Skill Engine)

### ğŸ¯ Goal  
Integrate the **Resume Parsing Engine (Day 30)** with the **Skill Extraction Engine (Day 29)** to automatically extract structured technical skills directly from uploaded PDF resumes.

This creates a complete **resume understanding pipeline**, a core feature of ATS and AI recruitment systems.

---

### ğŸ§© What I Built Today

#### 1ï¸âƒ£ **End-to-End Resume Skill Extraction Pipeline**
- Upload a PDF resume
- Automatically parse resume text
- Extract categorized technical skills
- Display results in a clean, recruiter-friendly UI

Pipeline flow:PDF Resume â†’ Parsed Text â†’ Skill Extraction â†’ Structured Skill Output

---

#### 2ï¸âƒ£ **Integrated Streamlit Application**
File:
job_classifier/resume_skill_extractor_streamlit.py

Features:
- PDF resume upload
- Automatic text parsing
- Skill extraction using NLP
- Side-by-side view:
  - Parsed resume text
  - Extracted skills by category

# Run command:
- ```bash
- streamlit run resume_skill_extractor_streamlit.py

ğŸ§  Relevance
- Demonstrates multi-module system integration
- Shows real-world ATS-style NLP workflow
- Strengthens evidence of applied AI expertise
- Adds depth to AI Recruitment Suite

## ğŸ—“ï¸ Day 32 â€” Experience Similarity Model (TF-IDF Baseline)

### ğŸ¯ Goal  
Add an **Experience Similarity Model** that measures how closely a candidateâ€™s resume experience matches a job description using **TF-IDF vectorization and cosine similarity**.

This module introduces **quantitative experience matching**, a core component of real-world ATS and AI recruitment systems.

---

### ğŸ§© What I Built Today

#### 1ï¸âƒ£ **Experience Similarity Engine**
- Uses TF-IDF vectorization (unigrams + bigrams)
- Computes cosine similarity between resume and job text
- Outputs a normalized percentage score (0â€“100)
- Fully explainable baseline model

File:job_classifier/experience_similarity.py

---

#### 2ï¸âƒ£ **Interactive Streamlit Interface**
File:job_classifier/experience_similarity_streamlit.py

Features:
- Side-by-side resume and job description inputs
- One-click experience similarity scoring
- Percentage score display
- Visual progress bar for match strength

# Run command:
- ```bash
- streamlit run experience_similarity_streamlit.py

## ğŸ—“ï¸ Day 33 â€” Unified Candidate Score (Skills + Experience + LLM)

### ğŸ¯ Goal  
Build a **Unified Candidate Scoring Engine** that combines multiple evaluation signals into a single, explainable score used for candidate ranking and shortlisting.

The unified score integrates:
- Skill Match Score  
- Experience Similarity Score  
- LLM-Based Reasoning Score  

This represents the **core decision logic** of the AI Recruitment System.

---

### ğŸ§© What I Built Today

#### 1ï¸âƒ£ **Unified Scoring Logic**
- Weighted scoring formula combining objective and AI-driven metrics
- Transparent and configurable weights
- Produces a normalized final score (0â€“100)

# Formula:
- Final Score =
- (Skill Match Ã— 0.4) +
- (Experience Similarity Ã— 0.4) +
- (LLM Reasoning Ã— 0.2)

# File:unified_score.py

---

#### 2ï¸âƒ£ **Interactive Streamlit Scoring Interface**
# File:unified_score_streamlit.py


Features:
- Adjustable sliders for each score component
- One-click unified score calculation
- Visual progress bar
- Score contribution breakdown for explainability

# Relevance
- Demonstrates system-level AI design
- Combines ML, NLP, and LLM reasoning
- Mirrors real-world ATS scoring pipelines
- Strengthens evidence of sustained original contribution

## ğŸ—“ï¸ Day 34 â€” Publish Medium Article #3 (Resume Parsing Insights)

## ğŸ—“ï¸ Day 35 â€” GitHub Release: Recruitment Suite v1.1

## ğŸ—“ï¸ Day 36 â€” AI Ranking Dashboard V2 (Interactive Charts)

### ğŸ¯ Goal  
Build **AI Ranking Dashboard V2**, an interactive visualization layer that enables recruiters to explore, rank, and analyze candidates using unified AI scores.

This dashboard transforms model outputs into **human-centered decision intelligence**.

---

### ğŸ§© What I Built Today

#### 1ï¸âƒ£ **Interactive Candidate Ranking Dashboard**
- Dynamic ranking based on unified AI score
- Threshold-based filtering
- Real-time updates via Streamlit

File: job_classifier/ai_ranking_dashboard_v2.py

---

#### 2ï¸âƒ£ **Visual Analytics**
- Bar chart comparing candidate scores
- Threshold reference line
- Score distribution histogram
- Combined table + chart view
---
# Run command:
- ```bash
- streamlit run ai_ranking_dashboard_v2.py
---
# ğŸ§ª Example Output
- Candidate A â€” 88%
- Candidate B â€” 81%
- Candidate C â€” 76%

---

## ğŸ“… Day 37 â€” Paper #3: Embedding-Based Job Matching (Full Technical Paper)

### ğŸ¯ Objective
Complete and publish a full research-style paper introducing an **embedding-based semantic matching engine** for AI-driven recruitment systems, extending beyond keyword and TF-IDF approaches.

### ğŸ§  What Was Done
- Authored a **complete technical paper** titled:  
  **â€œEmbedding-Based Semantic Matching for AI-Driven Recruitment Systemsâ€**
- The paper presents:
  - Motivation and limitations of traditional ATS systems
  - Semantic matching using text embeddings
  - System architecture integration
  - Evaluation against TF-IDF and keyword baselines
  - Explainability and recruiter-centric insights
- Structured as a **conference-ready research paper** with abstract, methodology, results, and future work.
- Written fully in **Markdown** for reproducibility and easy conversion to PDF.

### ğŸ“„ Artifacts Created
- `papers/embedding_job_matching.md`  
  (Full technical paper in Markdown format)

### ğŸ”— Repository
- GitHub:  
  https://github.com/konidinasricharan/retail-anomaly-poc

### ğŸ† Evidence Alignment
- **Original Contribution of Major Significance**
- **Scholarly Authorship**
- **Evidence of Advanced Expertise**
- **Research Publication (submission-ready)**

### ğŸ“Œ Notes
This paper strengthens the research narrative of the AI Recruitment Suite by introducing **semantic similarity modeling**, positioning the work closer to real-world ATS and enterprise hiring platforms. It is suitable for submission to workshops, conferences, arXiv, and DOI publication platforms.

---








